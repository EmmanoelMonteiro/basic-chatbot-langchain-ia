# basic-chatbot-langchain-ia
Chatbot com LLM Local (LM Studio, LangChain, Python)

## üìù Descri√ß√£o do Projeto
Este projeto apresenta um chatbot simples constru√≠do em Python utilizando a biblioteca LangChain para orquestra√ß√£o da conversa. A particularidade e grande vantagem deste chatbot √© que ele se comunica com um Large Language Model (LLM) que roda localmente na sua m√°quina, sem a necessidade de uma conex√£o com a internet para infer√™ncia (ap√≥s o download inicial do modelo).

A m√°gica de rodar o LLM localmente √© poss√≠vel gra√ßas ao LM Studio, uma ferramenta intuitiva que permite baixar e servir modelos de linguagem de c√≥digo aberto (como o Gemma-2-2B-IT, que estamos utilizando aqui) diretamente no seu computador, emulando uma API compat√≠vel com a OpenAI.

O objetivo principal deste projeto √© demonstrar a capacidade de:
* Configurar um ambiente de desenvolvimento para IA localmente.
* Integrar ferramentas como LM Studio e LangChain.
* Construir uma aplica√ß√£o de chatbot interativa via terminal.
* Explorar o poder dos LLMs que podem ser executados em hardware comum.

## ‚ú® Funcionalidades
* Intera√ß√£o de chatbot via terminal.
* Utiliza√ß√£o de um LLM rodando localmente (**Gemma-2-2B-IT**, configurado via LM Studio).
* Configura√ß√£o de ambiente virtual Python para isolamento de depend√™ncias.
* Fun√ß√£o de teste para verificar a comunica√ß√£o com o servidor LLM do LM Studio.

## üõ†Ô∏è Tecnologias Utilizadas
* **Python 3.9+**
* **LM Studio:** Ferramenta para baixar e rodar LLMs localmente.
* **LangChain:** Framework para desenvolvimento de aplica√ß√µes com LLMs.
* **langchain-openai:** Integra√ß√£o do LangChain com APIs compat√≠veis com OpenAI (usada para se conectar ao LM Studio).
* **python-dotenv:** Para gerenciar vari√°veis de ambiente (URL do servidor LLM).

## üöÄ Configura√ß√£o do Ambiente e Execu√ß√£o
Siga os passos abaixo para colocar o chatbot para rodar na sua m√°quina Windows 11.

### 1. Instala√ß√£o do LM Studio
1. Acesse o site oficial do LM Studio: https://lmstudio.ai/
2. Baixe e instale a vers√£o para Windows.

### 2. Download e Configura√ß√£o do Modelo no LM Studio
Ap√≥s instalar o LM Studio:

1. Abra o **LM Studio.**
2. Na barra lateral esquerda, clique na aba **"Search"** (√≠cone de lupa).
3. No campo de busca, digite `gemma-2-2b-it` e pressione Enter.
4. Procure por uma vers√£o **GGUF** do modelo (ex: `gemma-2-2b-it-q4_k_m.gguf`). Modelos com `Q4_K_M` ou `Q5_K_M` oferecem um bom equil√≠brio entre desempenho e consumo de RAM.
5. Clique no bot√£o **"Download"** ao lado da vers√£o escolhida. Aguarde o download ser conclu√≠do (pode levar um tempo dependendo da sua conex√£o).

### 3. Iniciando o Servidor Local do LLM no LM Studio
Com o modelo baixado:

1. No LM Studio, v√° para a aba **"Local Inference Server"** (√≠cone de duas setas, uma para cima e uma para baixo).
2. No painel esquerdo, certifique-se de que o modelo `gemma-2-2b-it` est√° selecionado no dropdown.
3. Verifique se a **porta padr√£o** (1234) est√° configurada. Anote a porta se for diferente.
4. Clique no bot√£o **"Start Server"**.
* Voc√™ ver√° uma mensagem como "Serving on `http://localhost:1234"` se o servidor iniciar corretamente.
* **Deixe o LM Studio rodando em segundo plano** enquanto voc√™ configura o Python.

### 4. Configura√ß√£o do Ambiente Python
1. Clone este reposit√≥rio para a sua m√°quina:
```Bash
git clone https://github.com/SeuUsuario/NomeDoSeuRepositorio.git
cd NomeDoSeuRepositorio
```
(Substitua `SeuUsuario/NomeDoSeuRepositorio.git` pelo caminho real do seu reposit√≥rio).

2. Crie um ambiente virtual (no diret√≥rio raiz do projeto):
```Bash
python -m venv .venv
```

3. Ative o ambiente virtual (no Windows):
```Bash
.venv\Scripts\activate
```
Voc√™ ver√° `(.venv)` no in√≠cio da linha de comando, indicando que o ambiente est√° ativado.

5. Cria√ß√£o do arquivo `requirements.txt`
Crie um arquivo chamado `requirements.txt` na raiz do seu projeto com o seguinte conte√∫do:

```
langchain
langchain-community
langchain-openai
python-dotenv
```

### 6. Instala√ß√£o das Depend√™ncias Python
Com o ambiente virtual ativado, instale as bibliotecas necess√°rias:

```Bash
pip install -r requirements.txt
```

### 7. Cria√ß√£o do arquivo `.env`
1. Crie um arquivo chamado `.env` na raiz do seu projeto (no mesmo n√≠vel do `main.py`).
2. Adicione a seguinte linha ao arquivo `.env`, **ajustando a porta se for diferente da padr√£o** `1234`:
```
LM_STUDIO_BASE_URL=http://localhost:1234/v1
```

### 8. Execu√ß√£o do Chatbot
Com o LM Studio servindo o modelo e o ambiente Python configurado:

1. No seu terminal, certifique-se de que voc√™ est√° na pasta raiz do projeto e que o ambiente virtual est√° ativado.
2. Execute o script principal:
```
python main.py
```

**Intera√ß√£o**
* O chatbot primeiro tentar√° conectar-se ao LLM. Se a conex√£o for bem-sucedida, ele iniciar√° o loop de conversa.
* Digite suas perguntas e pressione Enter.
* Para sair, digite sair e pressione Enter.

## üìÅ Estrutura do Projeto
```shell
.
‚îú‚îÄ‚îÄ .env                  # Vari√°veis de ambiente (URL do servidor LLM)
‚îú‚îÄ‚îÄ main.py               # Script principal do chatbot
‚îú‚îÄ‚îÄ requirements.txt      # Lista de depend√™ncias Python
‚îî‚îÄ‚îÄ README.md             # Este arquivo
```
